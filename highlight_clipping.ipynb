{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing YOLO package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTvK4CAq5U4o",
        "outputId": "01b1e176-8c03-433b-c799-7735146ef52a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Inference and Collect Highlights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkZCZMC9FnVO",
        "outputId": "2971eec0-94b9-4f4b-eb3f-102039cb5f67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 17658/17659 [16:21<00:00, 17.99it/s]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- Configuration ---\n",
        "VIDEO_PATH     = \"nuggets.mp4\"\n",
        "YOLO_PATH      = \"best_patrick_cp_1000.pt\"\n",
        "MODEL_WEIGHTS  = \"action_classifier_f.pth\"\n",
        "OUTPUT_PATH    = \"nuggets_highlight.mp4\"\n",
        "\n",
        "CLIP_LENGTH    = 15\n",
        "CONF_THRESHOLD = 0.4\n",
        "SMOOTH_WINDOW  = 6\n",
        "RESIZE_DIMS    = (128, 128)\n",
        "TARGET_FPS     = 30\n",
        "PADDING        = 20\n",
        "MARGIN_SECS    = 1\n",
        "MIN_DURATION   = 0.6  # seconds\n",
        "\n",
        "# --- Setup ---\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "FPS = round(cap.get(cv2.CAP_PROP_FPS))\n",
        "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "TOTAL_FRAMES = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load YOLO model\n",
        "yolo = YOLO(YOLO_PATH)\n",
        "\n",
        "# Load Action Recognizer\n",
        "actions = [\"none\", \"shoot\", \"layup\", \"dunk\"]\n",
        "transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize(RESIZE_DIMS),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "model = torchvision.models.video.r3d_18()\n",
        "model.fc = nn.Linear(model.fc.in_features, len(actions))\n",
        "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
        "model.to(device).eval()\n",
        "\n",
        "# Buffers\n",
        "raw_buffer = deque(maxlen=CLIP_LENGTH)\n",
        "detect_buffer = deque(maxlen=CLIP_LENGTH)\n",
        "score_buffer = deque(maxlen=SMOOTH_WINDOW)\n",
        "timestamps = []\n",
        "\n",
        "# --- Detection Helper ---\n",
        "def detect_bounding_box(frame):\n",
        "    result = yolo.predict(frame, imgsz=1280, conf=0.43, max_det=1, verbose=False)[0]\n",
        "    if not len(result.boxes):\n",
        "        return None\n",
        "\n",
        "    x1, y1, x2, y2 = result.boxes.xyxy[0].cpu().numpy().astype(int)\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    return (\n",
        "        max(0, x1 - PADDING),\n",
        "        max(0, y1 - PADDING),\n",
        "        min(w, x2 + PADDING),\n",
        "        min(h, y2 + PADDING)\n",
        "    )\n",
        "\n",
        "def classify_action(buffer):\n",
        "    batch = torch.stack(list(buffer)[::2], dim=1).unsqueeze(0).to(device)\n",
        "    logits = model(batch)\n",
        "    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "# --- Main Loop ---\n",
        "with torch.no_grad():\n",
        "    for frame_id in tqdm(range(0, TOTAL_FRAMES, FPS // TARGET_FPS)):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        box = detect_bounding_box(frame)\n",
        "        detect_buffer.append(box is not None)\n",
        "\n",
        "        if box is not None:\n",
        "            x1, y1, x2, y2 = box\n",
        "            cropped = transform(frame[y1:y2, x1:x2])\n",
        "            raw_buffer.append(cropped)\n",
        "        elif raw_buffer:\n",
        "            raw_buffer.append(raw_buffer[-1].clone())\n",
        "\n",
        "        if len(raw_buffer) == CLIP_LENGTH and any(detect_buffer):\n",
        "            probs = classify_action(raw_buffer)\n",
        "            score_buffer.append(1 - probs[0])  # confidence of any action\n",
        "            if len(score_buffer) == SMOOTH_WINDOW and np.mean(score_buffer) > CONF_THRESHOLD:\n",
        "                timestamps.append(frame_id)\n",
        "\n",
        "        elif not any(detect_buffer):\n",
        "            raw_buffer.clear()\n",
        "            score_buffer.clear()\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group and Merge Highlight Segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0dD-PbsIueo"
      },
      "outputs": [],
      "source": [
        "# --- Merge Clips ---\n",
        "duration = TOTAL_FRAMES / FPS if FPS > 0 else 0\n",
        "timestamps_sec = [f / FPS for f in timestamps]\n",
        "segments = []\n",
        "\n",
        "if timestamps_sec:\n",
        "    l = r = timestamps_sec[0]\n",
        "    for s in timestamps_sec[1:]:\n",
        "        if s - r < 2 * MARGIN_SECS:\n",
        "            r = s\n",
        "        else:\n",
        "            if r - l > MIN_DURATION:\n",
        "                start = max(0, l - MARGIN_SECS)\n",
        "                end = min(r + MARGIN_SECS, duration)\n",
        "                segments.append((start, end))\n",
        "            l = r = s\n",
        "    if r - l > MIN_DURATION:\n",
        "        start = max(0, l - MARGIN_SECS)\n",
        "        end = min(r + MARGIN_SECS, duration)\n",
        "        segments.append((start, end))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi_t1Kb-ZoQi",
        "outputId": "42b766bd-13ef-463d-9c5e-d1fcec54233e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(97.16666666666667, 99.8),\n",
              " (469.2, 472.23333333333335),\n",
              " (474.03333333333336, 478.06666666666666)]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Selected Highlight Clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Building video nuggets_highlight.mp4.\n",
            "MoviePy - Writing audio in nuggets_highlightTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing video nuggets_highlight.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done !\n",
            "MoviePy - video ready nuggets_highlight.mp4\n"
          ]
        }
      ],
      "source": [
        "from moviepy import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "OUTPUT_PATH = \"nuggets_highlight.mp4\"\n",
        "\n",
        "# --- Clip and Merge ---\n",
        "clips = []\n",
        "for start, end in segments:\n",
        "    clip = VideoFileClip(VIDEO_PATH).subclipped(start, end)\n",
        "    clips.append(clip)\n",
        "\n",
        "if clips:\n",
        "    final_clip = concatenate_videoclips(clips)\n",
        "    final_clip.write_videofile(OUTPUT_PATH, codec=\"libx264\")\n",
        "else:\n",
        "    print(\"No highlight clips found.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
